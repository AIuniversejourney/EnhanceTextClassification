{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T12:26:18.891596Z",
     "iopub.status.busy": "2023-05-27T12:26:18.891194Z",
     "iopub.status.idle": "2023-05-27T12:26:18.900300Z",
     "shell.execute_reply": "2023-05-27T12:26:18.899202Z",
     "shell.execute_reply.started": "2023-05-27T12:26:18.891555Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from math import log\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "import re\n",
    "from hazm import stopwords_list, word_tokenize \n",
    "from hazm import *\n",
    "import pandas\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.sparse as sp\n",
    "from flair.embeddings import TransformerDocumentEmbeddings, TransformerWordEmbeddings\n",
    "from flair.data import Sentence\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T12:26:23.557849Z",
     "iopub.status.busy": "2023-05-27T12:26:23.557415Z",
     "iopub.status.idle": "2023-05-27T12:26:23.606034Z",
     "shell.execute_reply": "2023-05-27T12:26:23.604758Z",
     "shell.execute_reply.started": "2023-05-27T12:26:23.557811Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from math import log\n",
    "\n",
    "def write_pickle(path: str, data: list) -> None:\n",
    "    \"\"\"\n",
    "    write_pickle function is written for write data in pickle file\n",
    "    :param path:\n",
    "    :param data:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    with open(path, \"wb\") as outfile:\n",
    "        pickle.dump(data, outfile)\n",
    "\n",
    "\n",
    "def read_pickle(path: str) -> list:\n",
    "    \"\"\"\n",
    "    read_pickle function for  reading pickle file\n",
    "    :param path:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    with open(path, \"rb\") as file:\n",
    "        data = pickle.load(file)\n",
    "    return data\n",
    "\n",
    "def preprocess_features(features):\n",
    "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
    "    rowsum = np.array(features.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    features = r_mat_inv.dot(features)\n",
    "    return features\n",
    "\n",
    "def create_features(sent_trans, sentences, train_size, test_size, tokenize_sentences, word_list, device):\n",
    "    sent_embs = []\n",
    "    word_embs = {}\n",
    "    len_of_samples = len(tokenize_sentences)\n",
    "    word_embedding_dict = {}\n",
    "\n",
    "    for ind in tqdm(range(train_size + test_size)):\n",
    "\n",
    "        sent = tokenize_sentences[ind]\n",
    "        sent_embs.append(sent_trans.encode([sentences[ind]])[0])\n",
    "        for word in sent:\n",
    "            if word in word_embs:\n",
    "                continue\n",
    "            try:\n",
    "                word_output = word_embedding_dict[word]\n",
    "            except:\n",
    "                word_output  = sent_trans.encode([word])[0]\n",
    "                word_embedding_dict[word] = word_output\n",
    "            word_embs[word] = word_output\n",
    "\n",
    "        if ind % 100 == 0:\n",
    "            print(ind, ' of ', len_of_samples, ' done...')\n",
    "    print(len(word_embs))\n",
    "    word_embs_list = []\n",
    "    for word in word_list:\n",
    "        word_embs_list.append(word_embs[word])\n",
    "\n",
    "    features = sent_embs[:train_size] + word_embs_list + sent_embs[train_size:]\n",
    "    features = preprocess_features(sp.csr_matrix(features)).todense()\n",
    "    features = torch.FloatTensor(features).to(device)\n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "def create_edge_attributes(edge, train_size, test_size, tokenize_sentences, word_id_map, word_list,\n",
    "                           vocab_length):\n",
    "    row = []\n",
    "    col = []\n",
    "    weight = []\n",
    "    if edge >= 1:\n",
    "        window_size = 20\n",
    "        total_W = 0\n",
    "        word_occurrence = {}\n",
    "        word_pair_occurrence = {}\n",
    "\n",
    "        def ordered_word_pair(a, b):\n",
    "            if a > b:\n",
    "                return b, a\n",
    "            else:\n",
    "                return a, b\n",
    "\n",
    "        def update_word_and_word_pair_occurrence(q):\n",
    "            unique_q = list(set(q))\n",
    "            for i in unique_q:\n",
    "                try:\n",
    "                    word_occurrence[i] += 1\n",
    "                except:\n",
    "                    word_occurrence[i] = 1\n",
    "            for i in range(len(unique_q)):\n",
    "                for j in range(i + 1, len(unique_q)):\n",
    "                    word1 = unique_q[i]\n",
    "                    word2 = unique_q[j]\n",
    "                    word1, word2 = ordered_word_pair(word1, word2)\n",
    "                    try:\n",
    "                        word_pair_occurrence[(word1, word2)] += 1\n",
    "                    except:\n",
    "                        word_pair_occurrence[(word1, word2)] = 1\n",
    "\n",
    "        for ind in tqdm(range(train_size + test_size)):\n",
    "            words = tokenize_sentences[ind]\n",
    "\n",
    "            q = []\n",
    "            # push the first (window_size) words into a queue\n",
    "            for i in range(min(window_size, len(words))):\n",
    "                q += [word_id_map[words[i]]]\n",
    "\n",
    "            # update the total number of the sliding windows\n",
    "            total_W += 1\n",
    "            # update the number of sliding windows that contain each word and word pair\n",
    "\n",
    "            update_word_and_word_pair_occurrence(q)\n",
    "\n",
    "            now_next_word_index = window_size\n",
    "            # pop the first word out and let the next word in, keep doing this until the end of the document\n",
    "            while now_next_word_index < len(words):\n",
    "                q.pop(0)\n",
    "                q += [word_id_map[words[now_next_word_index]]]\n",
    "                now_next_word_index += 1\n",
    "                # update the total number of the sliding windows\n",
    "                total_W += 1\n",
    "                # update the number of sliding windows that contain each word and word pair\n",
    "                update_word_and_word_pair_occurrence(q)\n",
    "\n",
    "        for word_pair in word_pair_occurrence:\n",
    "            i = word_pair[0]\n",
    "            j = word_pair[1]\n",
    "            count = word_pair_occurrence[word_pair]\n",
    "            word_freq_i = word_occurrence[i]\n",
    "            word_freq_j = word_occurrence[j]\n",
    "            pmi = log((count * total_W) / (word_freq_i * word_freq_j))\n",
    "            if pmi <= 0:\n",
    "                continue\n",
    "            row.append(train_size + i)\n",
    "            col.append(train_size + j)\n",
    "            weight.append(pmi)\n",
    "            row.append(train_size + j)\n",
    "            col.append(train_size + i)\n",
    "            weight.append(pmi)\n",
    "    # get each word appears in which document\n",
    "    word_doc_list = {}\n",
    "    for word in word_list:\n",
    "        word_doc_list[word] = []\n",
    "\n",
    "    for i in range(len(tokenize_sentences)):\n",
    "        doc_words = tokenize_sentences[i]\n",
    "        unique_words = set(doc_words)\n",
    "        for word in unique_words:\n",
    "            exsit_list = word_doc_list[word]\n",
    "            exsit_list.append(i)\n",
    "            word_doc_list[word] = exsit_list\n",
    "\n",
    "    # document frequency\n",
    "    word_doc_freq = {}\n",
    "    for word, doc_list in word_doc_list.items():\n",
    "        word_doc_freq[word] = len(doc_list)\n",
    "\n",
    "    # term frequency\n",
    "    doc_word_freq = {}\n",
    "\n",
    "    for doc_id in range(len(tokenize_sentences)):\n",
    "        words = tokenize_sentences[doc_id]\n",
    "        for word in words:\n",
    "            word_id = word_id_map[word]\n",
    "            doc_word_str = str(doc_id) + ',' + str(word_id)\n",
    "            if doc_word_str in doc_word_freq:\n",
    "                doc_word_freq[doc_word_str] += 1\n",
    "            else:\n",
    "                doc_word_freq[doc_word_str] = 1\n",
    "\n",
    "    for i in range(len(tokenize_sentences)):\n",
    "        words = tokenize_sentences[i]\n",
    "        doc_word_set = set()\n",
    "        for word in words:\n",
    "            if word in doc_word_set:\n",
    "                continue\n",
    "            j = word_id_map[word]\n",
    "            key = str(i) + ',' + str(j)\n",
    "            freq = doc_word_freq[key]\n",
    "            if i < train_size:\n",
    "                row.append(i)\n",
    "            else:\n",
    "                row.append(i + vocab_length)\n",
    "            col.append(train_size + j)\n",
    "            idf = log(1.0 * len(tokenize_sentences) / word_doc_freq[word_list[j]])\n",
    "            weight.append(freq * idf)\n",
    "            doc_word_set.add(word)\n",
    "\n",
    "    if edge >= 2:\n",
    "        tokenize_sentences_set = [set(s) for s in tokenize_sentences]\n",
    "        jaccard_threshold = 0.2\n",
    "        for i in tqdm(range(len(tokenize_sentences))):\n",
    "            for j in range(i + 1, len(tokenize_sentences)):\n",
    "                jaccard_w = 1 - nltk.jaccard_distance(tokenize_sentences_set[i],\n",
    "                                                      tokenize_sentences_set[j])\n",
    "                if jaccard_w > jaccard_threshold:\n",
    "                    if i < train_size:\n",
    "                        row.append(i)\n",
    "                    else:\n",
    "                        row.append(i + vocab_length)\n",
    "                    if j < train_size:\n",
    "                        col.append(j)\n",
    "                    else:\n",
    "                        col.append(vocab_length + j)\n",
    "                    weight.append(jaccard_w)\n",
    "                    if j < train_size:\n",
    "                        row.append(j)\n",
    "                    else:\n",
    "                        row.append(j + vocab_length)\n",
    "                    if i < train_size:\n",
    "                        col.append(i)\n",
    "                    else:\n",
    "                        col.append(vocab_length + i)\n",
    "                    weight.append(jaccard_w)\n",
    "\n",
    "    return row, col, weight\n",
    "\n",
    "\n",
    "def generate_train_val(train_size, vocab_length, node_size, train_pro=0.9):\n",
    "    real_train_size = int(train_pro * train_size)\n",
    "    val_size = train_size - real_train_size\n",
    "\n",
    "    idx_train = np.random.choice(train_size, real_train_size, replace=False)\n",
    "    idx_train.sort()\n",
    "    idx_val = []\n",
    "    pointer = 0\n",
    "    for v in range(train_size):\n",
    "        if pointer < len(idx_train) and idx_train[pointer] == v:\n",
    "            pointer += 1\n",
    "        else:\n",
    "            idx_val.append(v)\n",
    "    idx_test = range(train_size + vocab_length, node_size)\n",
    "\n",
    "    return idx_train, idx_val, idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T12:26:35.201748Z",
     "iopub.status.busy": "2023-05-27T12:26:35.201315Z",
     "iopub.status.idle": "2023-05-27T12:26:35.207937Z",
     "shell.execute_reply": "2023-05-27T12:26:35.206758Z",
     "shell.execute_reply.started": "2023-05-27T12:26:35.201709Z"
    }
   },
   "outputs": [],
   "source": [
    "EDGE = 2 # 0:d2w 1:d2w+w2w 2:d2w+w2w+d2d\n",
    "NODE = 1 # 0:one-hot #1:BERT \n",
    "NUM_LAYERS = 2 \n",
    "\n",
    "HIDDEN_DIM = 200\n",
    "DROP_OUT = 0.5\n",
    "LR = 0.02\n",
    "WEIGHT_DECAY = 0\n",
    "EARLY_STOPPING = 10\n",
    "NUM_EPOCHS = 200\n",
    "#normalizer = Normalizer()\n",
    "REMOVE_LIMIT = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digi_dataset = pandas.read_excel(\"digikala.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digi_dataset = digi_dataset[['comment','recommend']]\n",
    "print(digi_dataset.head())\n",
    "digi_dataset = digi_dataset[(digi_dataset['recommend'] == \"recommended\") | (digi_dataset['recommend'] == \"not_recommended\") | (digi_dataset['recommend'] == \"no_idea\") ]\n",
    "print(digi_dataset.shape)\n",
    "digi_dataset = digi_dataset.reset_index(drop=True )\n",
    "print(digi_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_word_freq = {}\n",
    "for sentence in list(digi_dataset[\"comment\"]):\n",
    "    word_list = word_tokenize(str(sentence))\n",
    "    for word in word_list:\n",
    "        if word in original_word_freq:\n",
    "            original_word_freq[word] += 1\n",
    "        else:\n",
    "            original_word_freq[word] = 1   \n",
    "\n",
    "print('done')\n",
    "\n",
    "limited_sentences = []\n",
    "limited_labels = []\n",
    "word_list_dict = {}\n",
    "for i,sentence in tqdm(enumerate(list(digi_dataset[\"comment\"]))):\n",
    "    word_list_temp = word_tokenize(str(sentence))\n",
    "    doc_words = []\n",
    "    for word in word_list_temp: \n",
    "        if word in original_word_freq and word not in stopwords_list() and original_word_freq[word] >= REMOVE_LIMIT:\n",
    "            doc_words.append(word)\n",
    "            word_list_dict[word] = 1\n",
    "    if len(doc_words) > 0:\n",
    "        limited_sentences.append(\" \".join(doc_words))\n",
    "        limited_labels.append(list(digi_dataset[\"recommend\"])[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(limited_sentences) == len(limited_labels)\n",
    "digi_dataset = pandas.DataFrame({\"comment\":limited_sentences,\"recommend\":limited_labels})\n",
    "digi_dataset.to_csv('digikala_data_remove20.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T12:26:44.180315Z",
     "iopub.status.busy": "2023-05-27T12:26:44.179365Z",
     "iopub.status.idle": "2023-05-27T12:26:44.512213Z",
     "shell.execute_reply": "2023-05-27T12:26:44.511166Z",
     "shell.execute_reply.started": "2023-05-27T12:26:44.180259Z"
    }
   },
   "outputs": [],
   "source": [
    "digi_dataset = pandas.read_csv(r\"C:/Users/AGM1/F.Gh/Digikala-3Class.csv\")\n",
    "digi_dataset = digi_dataset.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T12:26:48.109858Z",
     "iopub.status.busy": "2023-05-27T12:26:48.109287Z",
     "iopub.status.idle": "2023-05-27T12:26:48.134204Z",
     "shell.execute_reply": "2023-05-27T12:26:48.133183Z",
     "shell.execute_reply.started": "2023-05-27T12:26:48.109814Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>recommend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>سلام ، نظرم بگم میخواستم موضوع اشاره نظراتی کا...</td>\n",
       "      <td>recommended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>گیره های فلزی سخت میشوند حوله سخت توان _x 000 ...</td>\n",
       "      <td>not_recommended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>رابطه ظاهر گوشی . بدنه یکپارچه صفحه نمایش کیفی...</td>\n",
       "      <td>no_idea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ظرفیتش براتون کافیه حتما بخرید . _x 000 D_ یه ...</td>\n",
       "      <td>no_idea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>سلام دوستان ، ، _x 000 D_ منم مثه دوستان خرید ...</td>\n",
       "      <td>recommended</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment        recommend\n",
       "0  سلام ، نظرم بگم میخواستم موضوع اشاره نظراتی کا...      recommended\n",
       "1  گیره های فلزی سخت میشوند حوله سخت توان _x 000 ...  not_recommended\n",
       "2  رابطه ظاهر گوشی . بدنه یکپارچه صفحه نمایش کیفی...          no_idea\n",
       "3  ظرفیتش براتون کافیه حتما بخرید . _x 000 D_ یه ...          no_idea\n",
       "4  سلام دوستان ، ، _x 000 D_ منم مثه دوستان خرید ...      recommended"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digi_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T12:26:53.768742Z",
     "iopub.status.busy": "2023-05-27T12:26:53.768219Z",
     "iopub.status.idle": "2023-05-27T12:26:53.839780Z",
     "shell.execute_reply": "2023-05-27T12:26:53.838595Z",
     "shell.execute_reply.started": "2023-05-27T12:26:53.768694Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "nan_count = 0\n",
    "for item in list(digi_dataset.comment):\n",
    "     if type(item) is not str:\n",
    "        print(item, '*' * 10,type(item))\n",
    "        print('#' * 10)\n",
    "        nan_count += 1\n",
    "print(nan_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T12:26:57.022187Z",
     "iopub.status.busy": "2023-05-27T12:26:57.021252Z",
     "iopub.status.idle": "2023-05-27T12:26:57.060931Z",
     "shell.execute_reply": "2023-05-27T12:26:57.059767Z",
     "shell.execute_reply.started": "2023-05-27T12:26:57.022145Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50331 12583\n"
     ]
    }
   ],
   "source": [
    "original_train, original_test = train_test_split(digi_dataset, test_size = 0.2)\n",
    "\n",
    "original_train_sentences = list(original_train['comment'])\n",
    "original_labels_train = list(original_train['recommend'])\n",
    "\n",
    "original_test_sentences = list(original_test['comment'])\n",
    "original_labels_test = list(original_test['recommend'])\n",
    "\n",
    "\n",
    "train_size = len(original_train_sentences)\n",
    "test_size = len(original_test_sentences)\n",
    "sentences = original_train_sentences + original_test_sentences\n",
    "print(train_size, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T12:27:07.723968Z",
     "iopub.status.busy": "2023-05-27T12:27:07.723463Z",
     "iopub.status.idle": "2023-05-27T12:27:13.414970Z",
     "shell.execute_reply": "2023-05-27T12:27:13.413828Z",
     "shell.execute_reply.started": "2023-05-27T12:27:07.723929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no_idea' 'not_recommended' 'recommended']\n",
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "unique_labels=np.unique(original_labels_train)\n",
    "\n",
    "num_class = len(unique_labels)\n",
    "lEnc = LabelEncoder()\n",
    "lEnc.fit(unique_labels)\n",
    "\n",
    "print(unique_labels)\n",
    "print(lEnc.transform(unique_labels))\n",
    "\n",
    "train_labels = lEnc.transform(original_labels_train)\n",
    "test_labels = lEnc.transform(original_labels_test)\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "labels = train_labels.tolist()+test_labels.tolist()\n",
    "labels = torch.LongTensor(labels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T12:27:16.634130Z",
     "iopub.status.busy": "2023-05-27T12:27:16.633506Z",
     "iopub.status.idle": "2023-05-27T12:27:16.640500Z",
     "shell.execute_reply": "2023-05-27T12:27:16.639357Z",
     "shell.execute_reply.started": "2023-05-27T12:27:16.634075Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62914\n",
      "50331\n",
      "12583\n",
      "62914\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))\n",
    "print(len(train_labels))\n",
    "print(len(test_labels))\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T12:27:24.685577Z",
     "iopub.status.busy": "2023-05-27T12:27:24.684520Z",
     "iopub.status.idle": "2023-05-27T12:27:26.308770Z",
     "shell.execute_reply": "2023-05-27T12:27:26.307634Z",
     "shell.execute_reply.started": "2023-05-27T12:27:24.685534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62914it [00:00, 98890.63it/s] \n"
     ]
    }
   ],
   "source": [
    "original_word_freq = {}  # to remove rare words\n",
    "for sentence in sentences:\n",
    "    word_list = str(sentence).split()\n",
    "    for word in word_list:\n",
    "        if word in original_word_freq:\n",
    "            original_word_freq[word] += 1\n",
    "        else:\n",
    "            original_word_freq[word] = 1   \n",
    "\n",
    "print('done')\n",
    "\n",
    "tokenize_sentences = []\n",
    "word_list_dict = {}\n",
    "for i,sentence in tqdm(enumerate(sentences)):\n",
    "    word_list_temp = str(sentence).split()\n",
    "    doc_words = []\n",
    "    for word in word_list_temp: \n",
    "        if word in original_word_freq:\n",
    "            doc_words.append(word)\n",
    "            word_list_dict[word] = 1\n",
    "    tokenize_sentences.append(doc_words)\n",
    "\n",
    "word_list = list(word_list_dict.keys())\n",
    "vocab_length = len(word_list)\n",
    "\n",
    "\n",
    "word_id_map = {}\n",
    "for i in range(vocab_length):\n",
    "    word_id_map[word_list[i]] = i   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T12:27:29.957828Z",
     "iopub.status.busy": "2023-05-27T12:27:29.956807Z",
     "iopub.status.idle": "2023-05-27T12:27:29.963967Z",
     "shell.execute_reply": "2023-05-27T12:27:29.962635Z",
     "shell.execute_reply.started": "2023-05-27T12:27:29.957784Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70362\n"
     ]
    }
   ],
   "source": [
    "node_size = train_size + vocab_length + test_size\n",
    "print(node_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T12:27:40.354563Z",
     "iopub.status.busy": "2023-05-27T12:27:40.353596Z",
     "iopub.status.idle": "2023-05-27T12:27:44.662078Z",
     "shell.execute_reply": "2023-05-27T12:27:44.660960Z",
     "shell.execute_reply.started": "2023-05-27T12:27:40.354506Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading attr from local ...\n"
     ]
    }
   ],
   "source": [
    "create_edge_attributes_path = \"C:/Users/AGM1/F.Gh/attribute_Digikala-3Class.pickle\"\n",
    "\n",
    "if os.path.exists(create_edge_attributes_path):\n",
    "    print('loading attr from local ...')\n",
    "    ATTR = read_pickle(create_edge_attributes_path)\n",
    "    row, col, weight = ATTR[0], ATTR[1], ATTR[2]\n",
    "else:\n",
    "    row, col, weight = create_edge_attributes(edge=EDGE,\n",
    "                                              train_size=train_size,\n",
    "                                              test_size=test_size,\n",
    "                                              tokenize_sentences=tokenize_sentences,\n",
    "                                              word_list=word_list,\n",
    "                                              word_id_map=word_id_map,\n",
    "                                              vocab_length=vocab_length)\n",
    "    write_pickle(create_edge_attributes_path, data=[row, col, weight])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Parsbert_PATH = 'C:/Users/Downloads/Parsbert'\n",
    "Parsbert_MODEL = SentenceTransformer(Parsbert_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T12:29:13.728556Z",
     "iopub.status.busy": "2023-05-27T12:29:13.727576Z",
     "iopub.status.idle": "2023-05-27T12:29:14.074893Z",
     "shell.execute_reply": "2023-05-27T12:29:14.073808Z",
     "shell.execute_reply.started": "2023-05-27T12:29:13.728496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading features from local ...\n"
     ]
    }
   ],
   "source": [
    "features_saved_path = \"C:/Users/AGM1/F.Gh/features_Digikala-3Class.pickle\"\n",
    "\n",
    "if os.path.exists(features_saved_path):\n",
    "    print('loading features from local ...')\n",
    "    features = read_pickle(features_saved_path)\n",
    "\n",
    "else:\n",
    "\n",
    "    features = create_features(sent_trans  = Parsbert_MODEL,\n",
    "                                        sentences  = sentences,\n",
    "                                        train_size = train_size, \n",
    "                                        test_size  =  test_size,\n",
    "                                        tokenize_sentences=str(tokenize_sentences), \n",
    "                                        word_list=word_list, \n",
    "                                        device=device)\n",
    "    write_pickle(features_saved_path, data=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T18:23:17.931402Z",
     "iopub.status.busy": "2023-04-29T18:23:17.931032Z",
     "iopub.status.idle": "2023-04-29T18:23:17.937551Z",
     "shell.execute_reply": "2023-04-29T18:23:17.936444Z",
     "shell.execute_reply.started": "2023-04-29T18:23:17.931370Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70395, 512])\n",
      "50372 50364 0.8472357834625874\n",
      "14685859\n",
      "14685859\n",
      "14685859\n"
     ]
    }
   ],
   "source": [
    "print(features.shape)\n",
    "index = 123\n",
    "print(row[index], col[index], weight[index])\n",
    "print(len(row))\n",
    "print(len(col))\n",
    "print(len(weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T12:29:41.115459Z",
     "iopub.status.busy": "2023-05-27T12:29:41.114857Z",
     "iopub.status.idle": "2023-05-27T12:29:44.686894Z",
     "shell.execute_reply": "2023-05-27T12:29:44.685806Z",
     "shell.execute_reply.started": "2023-05-27T12:29:41.115409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 14685859])\n",
      "torch.Size([14685859, 1])\n"
     ]
    }
   ],
   "source": [
    "edge_index = [row, col]\n",
    "edge_attr = weight\n",
    "edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "print(edge_index.shape)\n",
    "edge_attr = torch.FloatTensor(edge_attr)\n",
    "edge_attr = edge_attr.reshape((len(weight), 1))\n",
    "print(edge_attr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T12:29:50.087584Z",
     "iopub.status.busy": "2023-05-27T12:29:50.086669Z",
     "iopub.status.idle": "2023-05-27T12:29:50.095104Z",
     "shell.execute_reply": "2023-05-27T12:29:50.093942Z",
     "shell.execute_reply.started": "2023-05-27T12:29:50.087540Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[70395, 512], edge_index=[2, 14685859], edge_attr=[14685859, 1])\n"
     ]
    }
   ],
   "source": [
    "data = Data(x=features, edge_index=edge_index, edge_attr=edge_attr)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T12:29:51.990779Z",
     "iopub.status.busy": "2023-05-27T12:29:51.990202Z",
     "iopub.status.idle": "2023-05-27T12:29:52.003790Z",
     "shell.execute_reply": "2023-05-27T12:29:52.002306Z",
     "shell.execute_reply.started": "2023-05-27T12:29:51.990728Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70362"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_size = train_size + vocab_length + test_size\n",
    "node_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T12:30:08.745732Z",
     "iopub.status.busy": "2023-05-27T12:30:08.745339Z",
     "iopub.status.idle": "2023-05-27T12:30:08.791483Z",
     "shell.execute_reply": "2023-05-27T12:30:08.790479Z",
     "shell.execute_reply.started": "2023-05-27T12:30:08.745697Z"
    }
   },
   "outputs": [],
   "source": [
    "idx_train, idx_val, idx_test = generate_train_val(train_size, vocab_length, node_size, train_pro=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ClusterData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T18:25:47.715678Z",
     "iopub.status.busy": "2023-04-29T18:25:47.715256Z",
     "iopub.status.idle": "2023-04-29T18:25:47.729518Z",
     "shell.execute_reply": "2023-04-29T18:25:47.728172Z",
     "shell.execute_reply.started": "2023-04-29T18:25:47.715642Z"
    }
   },
   "outputs": [],
   "source": [
    "train_mask = [0] * node_size\n",
    "for item in idx_train:\n",
    "    train_mask[item] = 1\n",
    "\n",
    "val_mask = [0] * node_size\n",
    "for item in idx_val:\n",
    "    val_mask[item] = 1\n",
    "\n",
    "test_mask = [0] * node_size\n",
    "for item in idx_test:\n",
    "    test_mask[item] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T18:25:17.217174Z",
     "iopub.status.busy": "2023-04-29T18:25:17.216144Z",
     "iopub.status.idle": "2023-04-29T18:25:17.226030Z",
     "shell.execute_reply": "2023-04-29T18:25:17.222764Z",
     "shell.execute_reply.started": "2023-04-29T18:25:17.217125Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[70395, 512], edge_index=[2, 14685859], edge_attr=[14685859, 1], train_mask=[70362], val_mask=[70362], test_mask=[70362])\n"
     ]
    }
   ],
   "source": [
    "data = Data(x=features, edge_index=edge_index, edge_attr=edge_attr, train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using ClusterData and graph partitioning with METIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-29T18:25:35.340477Z",
     "iopub.status.busy": "2023-04-29T18:25:35.339517Z",
     "iopub.status.idle": "2023-04-29T18:25:38.148106Z",
     "shell.execute_reply": "2023-04-29T18:25:38.144873Z",
     "shell.execute_reply.started": "2023-04-29T18:25:35.340439Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing METIS partitioning...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import ClusterData\n",
    "loader = ClusterData(data,num_parts = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in loader:\n",
    "    data.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[6834, 512], edge_attr=[179231, 1], train_mask=[70362], val_mask=[70362], test_mask=[70362], edge_index=[2, 179231])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Dropout\n",
    "from torch_geometric.nn import  GATConv, GCNConv, GINConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(data.num_node_features, 300)     # data.num_node_features = 512\n",
    "        self.conv2 = GCNConv(300, 100)\n",
    "        self.linear = torch.nn.Linear(100, 3)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.linear(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (conv1): GCNConv(512, 300)\n",
       "  (conv2): GCNConv(300, 100)\n",
       "  (linear): Linear(in_features=100, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GCN()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred_y, y):\n",
    "    \"\"\"Calculate accuracy.\"\"\"\n",
    "    return (pred_y == y).sum().item() / len(y)\n",
    "\n",
    "def train(model, optimizer, loader, labels, device):\n",
    "    model.train()\n",
    "    total_loss=0\n",
    "    train_loss=0\n",
    "    train_acc=0\n",
    "    val_loss=0\n",
    "    val_acc=0\n",
    "    \n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch) \n",
    "        \n",
    "        # Training\n",
    "        train_loss = F.nll_loss(out[batch.train_mask], labels[batch.train_mask])\n",
    "        train_acc  = accuracy(out[batch.train_mask].argmax(dim=1), labels[batch.train_mask])\n",
    "        \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        val_loss = F.nll_loss(out[batch.val_mask], labels[batch.val_mask])\n",
    "        val_acc  = accuracy(out[batch.val_mask].argmax(dim=1), labels[batch.val_mask])\n",
    "        \n",
    "        #==========================Loss=================================\n",
    "        train_loss += train_loss.item() \n",
    "        train_acc += train_acc\n",
    "        val_loss += val_loss.item()    \n",
    "        val_acc += val_acc\n",
    "        \n",
    "    train_loss = train_loss/len(loader)\n",
    "    train_acc  = train_acc/len(loader)\n",
    "    val_loss = val_acc/len(loader)\n",
    "    val_acc = val_acc / len(loader)\n",
    "# #     print('val_total_acc:', val_total_acc)\n",
    "# #     print('train_total_acc:', train_total_acc)\n",
    "# #     print(\"val_total_nodes:\", val_total_nodes)\n",
    "# #     print(\"train_total_nodes:\", train_total_nodes)\n",
    "    return train_loss, train_acc, val_loss, val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print metrics every 10 epochs\n",
    "device = 'cuda'\n",
    "\n",
    "model = GCN().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "for epoch in range(1000):\n",
    "    if(epoch % 10 == 0):\n",
    "        train_loss,val_loss, val_acc, train_acc= train (model, optimizer, loader, labels, device)\n",
    "        print(f'Epoch {epoch:>3} | Train Loss: {train_loss:.3f} | Train Acc:'\n",
    "                  f' {train_acc*100:>6.2f}% | Val Loss: {val_loss:.2f} | '\n",
    "                  f'Val Acc: {val_acc*100:.2f}%')\n",
    "        print('___________________________________________________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "if train_acc >= best_acc:\n",
    "        best_acc = train_acc\n",
    "torch.save(model, 'digi_best_model_gcn.pt')\n",
    "print('best saved model is :', best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GIN, self).__init__()\n",
    "        self.conv1 = GINConv(nn.Sequential(nn.Linear(data.num_node_features, 300), nn.ReLU(), nn.Linear(300, 300)))\n",
    "        self.conv2 = GINConv(nn.Sequential(nn.Linear(300, 100), nn.ReLU(), nn.Linear(100, 100)))\n",
    "        self.linear = nn.Linear(100, 3)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.linear(x)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred_y, y):\n",
    "    \"\"\"Calculate accuracy.\"\"\"\n",
    "    return (pred_y == y).sum().item() / len(y)\n",
    "\n",
    "def train(model, optimizer, loader, labels, device):\n",
    "    model.train()\n",
    "    total_loss=0\n",
    "    train_loss=0\n",
    "    train_acc=0\n",
    "    val_loss=0\n",
    "    val_acc=0\n",
    "    \n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch) \n",
    "        \n",
    "        # Training\n",
    "        train_loss = F.nll_loss(out[batch.train_mask], labels[batch.train_mask])\n",
    "        train_acc  = accuracy(out[batch.train_mask].argmax(dim=1), labels[batch.train_mask])\n",
    "        \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        val_loss = F.nll_loss(out[batch.val_mask], labels[batch.val_mask])\n",
    "        val_acc  = accuracy(out[batch.val_mask].argmax(dim=1), labels[batch.val_mask])\n",
    "        \n",
    "        #==========================Loss=================================\n",
    "        train_loss += train_loss.item() \n",
    "        train_acc += train_acc\n",
    "        val_loss += val_loss.item()    \n",
    "        val_acc += val_acc\n",
    "        \n",
    "    train_loss = train_loss/len(loader)\n",
    "    train_acc  = train_acc/len(loader)\n",
    "    val_loss = val_acc/len(loader)\n",
    "    val_acc = val_acc / len(loader)\n",
    "# #     print('val_total_acc:', val_total_acc)\n",
    "# #     print('train_total_acc:', train_total_acc)\n",
    "# #     print(\"val_total_nodes:\", val_total_nodes)\n",
    "# #     print(\"train_total_nodes:\", train_total_nodes)\n",
    "    return train_loss, train_acc, val_loss, val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print metrics every 10 epochs\n",
    "device = 'cuda'\n",
    "\n",
    "model = GIN().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "for epoch in range(1000):\n",
    "    if(epoch % 10 == 0):\n",
    "        train_loss,val_loss, val_acc, train_acc= train (model, optimizer, loader, labels, device)\n",
    "        print(f'Epoch {epoch:>3} | Train Loss: {train_loss:.3f} | Train Acc:'\n",
    "                  f' {train_acc*100:>6.2f}% | Val Loss: {val_loss:.2f} | '\n",
    "                  f'Val Acc: {val_acc*100:.2f}%')\n",
    "        print('___________________________________________________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "if train_acc >= best_acc:\n",
    "        best_acc = train_acc\n",
    "torch.save(model, 'digi_best_model_gin.pt')\n",
    "print('best saved model is :', best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, num_heads=1):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(data.num_node_features, 300, heads=num_heads)\n",
    "        self.conv2 = GATConv(300 * num_heads, 100, heads=num_heads)\n",
    "        self.linear = nn.Linear(100 * num_heads, 3)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.linear(x)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred_y, y):\n",
    "    \"\"\"Calculate accuracy.\"\"\"\n",
    "    return (pred_y == y).sum().item() / len(y)\n",
    "\n",
    "def train(model, optimizer, loader, labels, device):\n",
    "    model.train()\n",
    "    total_loss=0\n",
    "    train_loss=0\n",
    "    train_acc=0\n",
    "    val_loss=0\n",
    "    val_acc=0\n",
    "    \n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch) \n",
    "        \n",
    "        # Training\n",
    "        train_loss = F.nll_loss(out[batch.train_mask], labels[batch.train_mask])\n",
    "        train_acc  = accuracy(out[batch.train_mask].argmax(dim=1), labels[batch.train_mask])\n",
    "        \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        val_loss = F.nll_loss(out[batch.val_mask], labels[batch.val_mask])\n",
    "        val_acc  = accuracy(out[batch.val_mask].argmax(dim=1), labels[batch.val_mask])\n",
    "        \n",
    "        #==========================Loss=================================\n",
    "        train_loss += train_loss.item() \n",
    "        train_acc += train_acc\n",
    "        val_loss += val_loss.item()    \n",
    "        val_acc += val_acc\n",
    "        \n",
    "    train_loss = train_loss/len(loader)\n",
    "    train_acc  = train_acc/len(loader)\n",
    "    val_loss = val_acc/len(loader)\n",
    "    val_acc = val_acc / len(loader)\n",
    "# #     print('val_total_acc:', val_total_acc)\n",
    "# #     print('train_total_acc:', train_total_acc)\n",
    "# #     print(\"val_total_nodes:\", val_total_nodes)\n",
    "# #     print(\"train_total_nodes:\", train_total_nodes)\n",
    "    return train_loss, train_acc, val_loss, val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print metrics every 10 epochs\n",
    "device = 'cuda'\n",
    "\n",
    "model = GAT().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "for epoch in range(1000):\n",
    "    if(epoch % 10 == 0):\n",
    "        train_loss,val_loss, val_acc, train_acc= train (model, optimizer, loader, labels, device)\n",
    "        print(f'Epoch {epoch:>3} | Train Loss: {train_loss:.3f} | Train Acc:'\n",
    "                  f' {train_acc*100:>6.2f}% | Val Loss: {val_loss:.2f} | '\n",
    "                  f'Val Acc: {val_acc*100:.2f}%')\n",
    "        print('___________________________________________________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "if train_acc >= best_acc:\n",
    "        best_acc = train_acc\n",
    "torch.save(model, 'digi_best_model_gat.pt')\n",
    "print('best saved model is :', best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GCN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(data.num_node_features, 300)     # data.num_node_features = 512\n",
    "        self.conv2 = GCNConv(300, 100)\n",
    "        self.linear = torch.nn.Linear(100, 3)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data[3].x, data[0].edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.linear(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred_y, y):\n",
    "    \"\"\"Calculate accuracy.\"\"\"\n",
    "    return (pred_y == y).sum().item() / len(y)\n",
    "\n",
    "def train(model, optimizer, loader, labels, device):\n",
    "    model.train()\n",
    "    total_loss=0\n",
    "    train_loss=0\n",
    "    train_acc=0\n",
    "    val_loss=0\n",
    "    val_acc=0\n",
    "    \n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch) \n",
    "        \n",
    "        # Training\n",
    "        train_loss = F.nll_loss(out[batch.train_mask], labels[batch.train_mask])\n",
    "        train_acc  = accuracy(out[batch.train_mask].argmax(dim=1), labels[batch.train_mask])\n",
    "        \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        val_loss = F.nll_loss(out[batch.val_mask], labels[batch.val_mask])\n",
    "        val_acc  = accuracy(out[batch.val_mask].argmax(dim=1), labels[batch.val_mask])\n",
    "        \n",
    "        #==========================Loss=================================\n",
    "        train_loss += train_loss.item() \n",
    "        train_acc += train_acc\n",
    "        val_loss += val_loss.item()    \n",
    "        val_acc += val_acc\n",
    "        \n",
    "    train_loss = train_loss/len(loader)\n",
    "    train_acc  = train_acc/len(loader)\n",
    "    val_loss = val_acc/len(loader)\n",
    "    val_acc = val_acc / len(loader)\n",
    "# #     print('val_total_acc:', val_total_acc)\n",
    "# #     print('train_total_acc:', train_total_acc)\n",
    "# #     print(\"val_total_nodes:\", val_total_nodes)\n",
    "# #     print(\"train_total_nodes:\", train_total_nodes)\n",
    "    return train_loss, train_acc, val_loss, val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print metrics every 10 epochs\n",
    "device = 'cuda'\n",
    "\n",
    "model = GCN2().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "for epoch in range(1000):\n",
    "    if(epoch % 10 == 0):\n",
    "        train_loss,val_loss, val_acc, train_acc= train (model, optimizer, loader, labels, device)\n",
    "        print(f'Epoch {epoch:>3} | Train Loss: {train_loss:.3f} | Train Acc:'\n",
    "                  f' {train_acc*100:>6.2f}% | Val Loss: {val_loss:.2f} | '\n",
    "                  f'Val Acc: {val_acc*100:.2f}%')\n",
    "        print('___________________________________________________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "if train_acc >= best_acc:\n",
    "        best_acc = train_acc\n",
    "torch.save(model, 'digi_best_model_gcn2.pt')\n",
    "print('best saved model is :', best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gcn = torch.load('digi_best_model_gcn.pt')\n",
    "model_gin = torch.load('digi_best_model_gin.pt')\n",
    "model_gat = torch.load('digi_best_model_gat.pt')\n",
    "model_gcn2 = torch.load('digi_best_model_gcn2.pt')\n",
    "\n",
    "model_gcn.eval()\n",
    "model_gin.eval()\n",
    "model_gat.eval()\n",
    "model_gcn2.eval()\n",
    "\n",
    "acc_gcn = 0\n",
    "acc_gin = 0\n",
    "acc_gat = 0\n",
    "acc_gcn2 = 0\n",
    "acc = 0\n",
    "for batch in loader:\n",
    "       \n",
    "    batch = batch.to(device)\n",
    "       \n",
    "    optimizer.zero_grad()\n",
    "    out_gcn = model_gcn(batch)\n",
    "    out_gin = model_gin(batch)\n",
    "    out_gat = model_gat(batch)\n",
    "    out_gcn2 = model_gcn2(batch)\n",
    "    out = (out_gcn + out_gin + out_gat + out_gcn2)/4\n",
    "       \n",
    "    acc_gcn += accuracy(out_gcn[batch.test_mask].argmax(dim=1),labels[batch.test_mask])/ len(loader)\n",
    "    acc_gin += accuracy(out_gin[batch.test_mask].argmax(dim=1),labels[batch.test_mask])/ len(loader)\n",
    "    acc_gat += accuracy(out_gat[batch.test_mask].argmax(dim=1),labels[batch.test_mask])/ len(loader)\n",
    "    acc_gcn2 += accuracy(out_gcn2[batch.test_mask].argmax(dim=1),labels[batch.test_mask])/ len(loader)\n",
    "    acc_predictions = out[batch.test_mask].argmax(dim=1).cpu()\n",
    "    acc_labels = labels[batch.test_mask].cpu()\n",
    "   \n",
    "    acc += accuracy(acc_labels, acc_predictions)/ len(loader)\n",
    "   \n",
    "    precision += precision_score(acc_labels, acc_predictions, average='weighted')/ len(loader)\n",
    "    recall += recall_score(acc_labels, acc_predictions, average='weighted')/ len(loader)\n",
    "    f1 += f1_score(acc_labels, acc_predictions, average='weighted')/ len(loader)\n",
    "       \n",
    "\n",
    "# Print results\n",
    "print(f'GCN accuracy:     {acc_gcn*100:.2f}%')\n",
    "print(f'GAT accuracy:     {acc_gat*100:.2f}%')\n",
    "print(f'GIN accuracy:     {acc_gat*100:.2f}%')\n",
    "print(f'GCN2 accuracy:     {acc_gat*100:.2f}%')\n",
    "\n",
    "print(f'Overall accuracy: {acc*100:.2f}%')\n",
    "print(f'Precision:         {precision*100:.2f}%')\n",
    "print(f'Recall:            {recall*100:.2f}%')\n",
    "print(f'F1-score:          {f1*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
